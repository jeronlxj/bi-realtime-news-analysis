Project Requirements Document:
1. Project Introduction
In this project, you need to implement a news real-time dynamic change analysis system to provide PENS data with real-time dynamic change analysis of news topics, historical/real-time statistics of various news, and news topic recommendations. The modules that need to be implemented are as follows:
1) News data and clickstream data simulation: Using the news exposure log (Impression Log) provided in PENS, a simulation log generation program is constructed for the news exposure log to simulate the time series log generation.
2) ETL: Use the Flume framework to collect the news exposure logs generated by the simulation in the previous step. After the ETL step, the data needs to be put into the Kafka queue for flow control, and then stored in the storage system you defined. At the same time, obtain the news data in PENS.
3) Storage System: This module should complete the data storage (various relational or non-relational storage can be used) and provide high-performance query services for Query Server. In the storage process, the rationality of data preprocessing, data modeling, indexing, etc. should be considered to improve efficiency and system stability and achieve system performance indicators.
4) Analysis Server: This module provides intelligent query and analysis services for the front-end data, using Spark Streaming technology and an analysis method based on AI+BI Agent. It should provide the retrieval and analysis capabilities for news dynamics that the front-end needs to achieve.
5) UI: This module is for front-end data visualization. It is necessary to develop queries and analysis for news dynamic changes and reasonably visualize the results to achieve quick access to the result set.
2. Dataset
The data in this project comes from the PENS dataset. The dataset includes news, news exposure logs, and other information. You need to familiarize yourself with this dataset.
The PENS news corpus contains about 110,000 English news articles. Each news article consists of four parts: news ID, news title, news text, and news category label. All news that appear in the training and test data correspond to the news ID of the article in the corpus.
The training data set of PENS contains news exposure logs of anonymous users, including 500,000 news exposure logs of 440,000 anonymous users, as well as historical click information of each user. Specifically, each piece of training data consists of five parts: user ID, exposure timestamp, clicked news list, unclicked news list, and user's historical clicked news list. All news appearing in the list are sorted by the time of first exposure.
In order to meet the needs of offline evaluation, the researchers invited 103 native English-speaking college students (hereinafter referred to as "annotators") to manually create the PENS test data set. The construction process is divided into two stages: in the first stage, each annotator browses 1,000 news headlines randomly selected from the news corpus and selects at least 50 headlines of interest to him/her, which are regarded as the user's historical click behavior; in the second stage, each annotator writes the ideal headline in his/her mind for another 200 news articles. These manually written news headlines are reviewed for quality by professional news editors. Low-quality headlines will be deleted (for example, too long, too short, or inconsistent with the text), and the remaining qualified headlines will serve as the gold standard for the corresponding user's personalized news headlines.
3. Scoring points
In this project, you need to implement the data acquisition, cleaning, storage, query and visualization process. In each module, you can use your ability to think about and optimize the key points of system design. Each optimization point will improve the score. The detailed scoring rules are as follows:
1) Basic functions: You need to implement at least the following functions: reading and importing data sets, completing basic functions including but not limited to news clickstream data generation, parsing, storage, statistics, analysis, and display, and being able to display the front-end in a visual way according to user query and analysis requirements.
2) Query and analysis: You need to implement at least the following basic queries:
2.1. Querying the life cycle of a single piece of news can show the popularity changes of a single piece of news in different time periods.
2.2. Statistical query on the changes of certain types of news can display different categories
2.3. Statistical query on changes in user interests
2.4. Statistical query can be performed according to various conditions and combinations such as time/time period, news topic, news title length, news length, specific user, specific multiple users, etc.
2.5. Ability to analyze what kind of news is most likely to become a hot news
2.6. Ability to recommend news in real time based on the content browsed by the user
2.7. It is necessary to establish a query log to record all SQL query records and query time to facilitate the inspection and optimization of performance indicators
3) Query performance requirements:
3.1. Any single query above needs to be completed within 1 second and the front-end result rendering
3.2. In the front-end page, a page may contain multiple static or dynamic query results (for example, the page may contain query changes according to different news topics (15 types) every day (2019-6-13 to 2019-7-3), etc.). A single page also needs to complete the query and front-end result rendering within 3 seconds.

